

LDBC-SNB Test Driver is in charge of the execution of the Interactive Workload.
At the begining of the execution, the Test Driver creates a query mix by
assigning to each query instance, a query issue time and a set of parameters
taken from the generated substitution parameter set described above.  

Query issue times have to be carefully assigned.  Although substitution
parameters are choosen in such a way that queries of the same type take similar
time, not all query types have the same complexity and touch the same amount of
data. Therefore, if all query instances, regardless of their type, are issued
at the same rate, those more complex queries will dominate the execution's
result, making faster query types purposeless. To avoid this situation, each
query type is assigned a different interleave time. This interleave time
corresponds to the amount of time that must elapse between issuing two query
instances of the same type. Interleave times have been empirically determined
by experimenting with the workload on different existing database technologies.
Those more complex query types, will have larger interleave times than those
faster queries. Table~\ref{table:interleaved} shows the current interleave times 
assigned to each query type in milliseconds.

\begin{table}[H]
\centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    Query Type & ms & Query Type & ms \\ 
    \hline
    \hline
    Query 1 & 30 & Query 8 & 1 \\ 
    \hline       
    Query 2 & 12 & Query 9 & 40 \\  
    \hline        
    Query 3 & 72 & Query 10 & 27 \\ 
    \hline        
    Query 4 & 27 & Query 11 & 18 \\ 
    \hline        
    Query 5 & 42 & Query 12 & 34 \\ 
    \hline        
    Query 6 & 18 & Query 13 & 1 \\  
    \hline        
    Query 7 & 13 & Query 14 & 66 \\ 
    \hline
    \end{tabular}
    \caption{Interleaved latencies for each query type in miliseconds.}
    \label{table:interleaved}
\end{table}

The specified interleave times, implicitly define the query ratios between
queries of different types, as well as a default target throughput. However the
Test Sponsor may specify a different target throughput to test,  by
`squeezing'' together or ``stretching'' further apart the queries of the
workload. This is achieved  by means of a factor that is multiplied by the
interleaved latencies (see \ref{ssub:general_driver_properties}).  Therefore,
different throughputs can be tested while maintaining the relative ratios
between the query types.

%Rather than simply measuring the maximum throughput an SUT can achieve given some
%query mix, the Interactive Workload explicitly defines the rate that queries
%will be sent to the SUT, and query response times (latencies) are then measured
%for these queries.  
%
%In addition to specifying query types and parameters, the Interactive Workload
%definition also includes the target load.  That is, the frequency/throughput at
%which queries will be executed.
%
%This philosophy is more in line with the
%\textit{interactive} nature of the benchmark.  Further, specifying a particular
%query rate allows the benchmark to measure how an SUT performs, not only with a
%given set of query types, but also under under a given load.
%
%More specifically, the Interactive Workload includes two distinct groups of queries,
%reads and writes.  Although writes will be disabled for the initial benchmark
%release, it is important to understand how reads and writes relate to one
%another, and to be aware of the difference between ``simulation time'' and
%``real time''.
%
%Both groups (reads and writes) are loaded from ``event stream'' files that
%DBGEN produces, but there is one important distinction between them:
%the files for reads contain substitution parameters only, the files for writes
%contain both substitution parameters and timestamps that indicate the scheduled
%execution time of each operation.  One of the reasons writes include timestamps
%is they are dependent on one another, that is, to ensure data consistency they
%must be executed in the correct order.
%
%Further, due to the fact that write timestamps are created in the data
%generator, they map to what we refer to as ``simulation time'': the timestamps
%may be in the past or future, but are always between the start and end times of
%the generated dataset; the scheduled execution time of an ``add post''
%operation, for example, will be identical to the \texttt{creationDate}
%attribute of that same \texttt{Post} record.  An example entry in the write
%stream file is as follows:
%
%\vspace{-6mm}
%$$
%1293907146704\|ADD\_LIKE\_POST\|[31277,42949,``2011-01-01T19:39:06Z'']\|
%$$
%
%On the other hand, as the data generator provides no associated timestamps for
%the read queries these must be created in the workload driver.  As such, the
%driver provides parameters to control how timestamps are generated.  The
%duration (in ``simulation time'') between queries can be set, individually per
%query type.  Using these settings the driver generates multiple streams of read
%queries, on per query type, then merges them, along with the write stream
%provided by the data generator, into one time-ordered stream of read and write
%operations.
%
%Finally, to provide a means of increasing or decreasing the
%frequency/throughput of operations issued by the workload driver, the driver
%provides a ``time compression'' (see \ref{ssub:general_driver_properties})
%mechanism allowing timestamps in the generated operation stream to be
%``squeezed'' closer together or ``stretched'' further apart, thereby
%controlling the rate at which they are executed.
